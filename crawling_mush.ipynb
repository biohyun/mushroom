{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Can not find chromedriver for currently installed chrome version.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "def fetch_image_urls(query, max_links_to_fetch, browser):\n",
    "    browser.get('https://www.google.com/imghp')\n",
    "    search_box = browser.find_element(By.NAME, \"q\")\n",
    "\n",
    "\n",
    "    search_box.send_keys(query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait until page is loaded\n",
    "    time.sleep(2)\n",
    "\n",
    "    image_urls = set()\n",
    "    thumbnail_results = browser.find_elements(By.CSS_SELECTOR, \"img.Q4LuWd\")\n",
    "\n",
    "    number_results = min(max_links_to_fetch, len(thumbnail_results))\n",
    "\n",
    "    for img in thumbnail_results[:number_results]:\n",
    "        try:\n",
    "            img.click()\n",
    "            time.sleep(1)\n",
    "            actual_images = browser.find_elements(By.CSS_SELECTOR, 'img.n3VNCb')\n",
    "            for actual_image in actual_images:\n",
    "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                    image_urls.add(actual_image.get_attribute('src'))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "\n",
    "def search_and_download(search_query, number_images):\n",
    "    browser = webdriver.Chrome()\n",
    "    urls = fetch_image_urls(search_query, number_images, browser)\n",
    "\n",
    "    # You can use these URLs to download the images using the `requests` library.\n",
    "\n",
    "    browser.quit()\n",
    "    return urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def download_images(image_urls, save_directory=\".\"):\n",
    "    \"\"\"\n",
    "    Download images from the provided URLs and save them to the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        image_urls (set): A set of image URLs to download.\n",
    "        save_directory (str): Directory to save the downloaded images. Default is the current directory.\n",
    "        \n",
    "    Returns:\n",
    "        List of file paths where images were saved.\n",
    "    \"\"\"\n",
    "    saved_files = []\n",
    "    \n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    for i, url in enumerate(image_urls):\n",
    "        response = requests.get(url, stream=True)\n",
    "        file_name = os.path.join(save_directory, f\"img_{i}.jpg\")\n",
    "\n",
    "        with open(file_name, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 8):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        saved_files.append(file_name)\n",
    "\n",
    "    return saved_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images...\n",
      "Downloaded 0 images:\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "image_urls = search_and_download(\"mushroom\", 5)\n",
    "print(\"Downloading images...\")\n",
    "saved_files = download_images(image_urls, \"downloaded_images\")\n",
    "print(f\"Downloaded {len(saved_files)} images:\")\n",
    "for file_path in saved_files:\n",
    "    print(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Can not find chromedriver for currently installed chrome version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "def scroll_to_load_images(browser, num_scrolls):\n",
    "    \"\"\"Scroll down the page to load more images.\"\"\"\n",
    "    for _ in range(num_scrolls):\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "def fetch_image_urls(query, max_links_to_fetch, browser):\n",
    "    browser.get('https://www.google.com/imghp')\n",
    "    search_box = browser.find_element(By.NAME, \"q\")\n",
    "    search_box.send_keys(query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Scroll a few times to load more images\n",
    "    scroll_to_load_images(browser, 3)\n",
    "\n",
    "    image_urls = set()\n",
    "    thumbnail_results = browser.find_elements(By.CSS_SELECTOR, \"img.Q4LuWd\")\n",
    "    number_results = min(max_links_to_fetch, len(thumbnail_results))\n",
    "\n",
    "    for img in thumbnail_results[:number_results]:\n",
    "        try:\n",
    "            img.click()\n",
    "            time.sleep(1)\n",
    "            actual_images = browser.find_elements(By.CSS_SELECTOR, 'img.n3VNCb')\n",
    "            for actual_image in actual_images:\n",
    "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                    image_urls.add(actual_image.get_attribute('src'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "def search_and_download(search_query, number_images):\n",
    "    browser = webdriver.Chrome()\n",
    "    urls = fetch_image_urls(search_query, number_images, browser)\n",
    "    browser.quit()\n",
    "    return urls\n",
    "\n",
    "# Example\n",
    "image_urls = search_and_download(\"mushroom\", 5)\n",
    "print(image_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down():\n",
    "    global driver\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            time.sleep(1)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            try:\n",
    "                driver.find_element_by_class_name(\"mye4qd\").click()\n",
    "            except:\n",
    "\n",
    "               if new_height == last_height:\n",
    "                   break\n",
    "\n",
    "\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of img tags:  396\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "keyword = input('검색할 태그를 입력하세요 : ')\n",
    "url = 'https://www.google.com/search?q={}&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjgwPKzqtXuAhWW62EKHRjtBvcQ_AUoAXoECBEQAw&biw=768&bih=712'.format(keyword)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "scroll_down()\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "images = soup.find_all('img', attrs={'class':'rg_i Q4LuWd'})\n",
    "\n",
    "print('number of img tags: ', len(images))\n",
    "\n",
    "n = 1\n",
    "for i in images:\n",
    "\n",
    "    try:\n",
    "        imgUrl = i[\"src\"]\n",
    "    except:\n",
    "        imgUrl = i[\"data-src\"]\n",
    "        \n",
    "    with urllib.request.urlopen(imgUrl) as f:\n",
    "        with open('./mushroom_crawling/' + keyword + str(n) + '.png', 'wb') as h:\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "\n",
    "def lets_crawling(keyword,num_image):\n",
    "\n",
    "    keyword_search = keyword.replace('_', ' ')+ ' mushroom'\n",
    "    url = 'https://www.google.com/search?q={}&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjgwPKzqtXuAhWW62EKHRjtBvcQ_AUoAXoECBEQAw&biw=768&bih=712'.format(keyword_search)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    scroll_down()\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    images = soup.find_all('img', attrs={'class':'rg_i Q4LuWd'})\n",
    "\n",
    "    print('number of img tags: ', len(images))\n",
    "\n",
    "            # Define the path for the new folder\n",
    "    folder_path = f\"./mushroom_crawling/{keyword}/\"\n",
    "\n",
    "    # Check if the folder already exists; if not, create it\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder {folder_path} created!\")\n",
    "    else:\n",
    "        print(f\"Folder {folder_path} already exists!\")\n",
    "\n",
    "\n",
    "    n = 1\n",
    "    for i in images:\n",
    "\n",
    "        try:\n",
    "            imgUrl = i[\"src\"]\n",
    "        except:\n",
    "            imgUrl = i[\"data-src\"]\n",
    "\n",
    "            \n",
    "        with urllib.request.urlopen(imgUrl) as f:\n",
    "            with open(folder_path + keyword +'_'+ str(n) + '.png', 'wb') as h:\n",
    "                img = f.read()\n",
    "                h.write(img)\n",
    "\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['almond_mushroom', 'amanita_gemmata', 'amethyst_chanterelle', 'amethyst_deceiver', 'aniseed_funnel', 'ascot_hat', 'bay_bolete', 'bearded_milkcap', 'beechwood_sickener', 'beefsteak_fungus', 'birch_polypore', 'birch_woodwart', 'bitter_beech_bolete', 'bitter_bolete', 'blackening_brittlegill', 'blackening_polypore', 'blackening_waxcap', 'black_bulgar', 'black_morel', 'blue_roundhead', 'blushing_bracket', 'blushing_rosette', 'blushing_wood_mushroom', 'bovine_bolete', 'bronze_bolete', 'brown_birch_bolete', 'brown_rollrim', 'bruising_webcap', 'butter_cap', 'cauliflower_fungus', 'cedarwood_waxcap', 'chanterelle', 'charcoal_burner', 'chestnut_bolete', 'chicken_of_the_woods', 'cinnamon_bracket', 'clouded_agaric', 'clustered_domecap', 'common_bonnet', 'common_inkcap', 'common_morel', 'common_puffball', 'common_rustgill', 'crimped_gill', 'crimson_waxcap', 'cucumber_cap', 'curry_milkcap', 'deadly_fibrecap', 'deadly_webcap', 'deathcap', 'deer_shield', 'destroying_angel', 'devils_bolete', 'dog_stinkhorn', 'dryads_saddle', 'dusky_puffball', 'dyers_mazegill', 'earthballs', 'egghead_mottlegill', 'elfin_saddle', 'fairy_ring_champignons', 'false_chanterelle', 'false_deathcap', 'false_morel', 'false_saffron_milkcap', 'fenugreek_milkcap', 'field_blewit', 'field_mushroom', 'fleecy_milkcap', 'fly_agaric', 'fools_funnel', 'fragrant_funnel', 'freckled_dapperling', 'frosted_chanterelle', 'funeral_bell', 'geranium_brittlegill', 'giant_funnel', 'giant_puffball', 'glistening_inkcap', 'golden_bootleg', 'golden_scalycap', 'golden_waxcap', 'greencracked_brittlegill', 'grey_knight', 'grey_spotted_amanita', 'grisettes', 'hairy_curtain_crust', 'heath_waxcap', 'hedgehog_fungus', 'hen_of_the_woods', 'honey_fungus', 'hoof_fungus', 'horn_of_plenty', 'horse_mushroom', 'inky_mushroom', 'jelly_ears', 'jelly_tooth', 'jubilee_waxcap', 'king_alfreds_cakes', 'larch_bolete', 'leccinum_albostipitatum', 'liberty_cap', 'lilac_bonnet', 'lilac_fibrecap', 'lions_mane', 'lurid_bolete', 'macro_mushroom', 'magpie_inkcap', 'meadow_waxcap', 'medusa_mushroom', 'morel', 'mosaic_puffball', 'oak_bolete', 'oak_mazegill', 'oak_polypore', 'ochre_brittlegill', 'old_man_of_the_woods', 'orange_birch_bolete', 'orange_bolete', 'orange_grisette', 'orange_peel_fungus', 'oyster_mushroom', 'pale_oyster', 'panthercap', 'parasol', 'parrot_waxcap', 'pavement_mushroom', 'penny_bun', 'peppery_bolete', 'pestle_puffball', 'pine_bolete', 'pink_waxcap', 'plums_and_custard', 'poison_pie', 'poplar_bell', 'poplar_fieldcap', 'porcelain_fungus', 'powdery_brittlegill', 'purple_brittlegill', 'red_belted_bracket', 'red_cracking_bolete', 'rooting_bolete', 'rooting_shank', 'root_rot', 'rosy_bonnet', 'ruby_bolete', 'saffron_milkcap', 'scaly_wood_mushroom', 'scarletina_bolete', 'scarlet_caterpillarclub', 'scarlet_elfcup', 'scarlet_waxcap', 'semifree_morel', 'sepia_bolete', 'shaggy_bracket', 'shaggy_inkcap', 'shaggy_parasol', 'shaggy_scalycap', 'sheathed_woodtuft', 'silky_rosegill', 'silverleaf_fungus', 'slender_parasol', 'slimy_waxcap', 'slippery_jack', 'smoky_bracket', 'snakeskin_grisette', 'snowy_waxcap', 'spectacular_rustgill', 'splendid_waxcap', 'splitgill', 'spotted_toughshank', 'spring_fieldcap', 'stinkhorn', 'stinking_dapperling', 'stubble_rosegill', 'stump_puffball', 'st_georges_mushroom', 'suede_bolete', 'sulphur_tuft', 'summer_bolete', 'tawny_funnel', 'tawny_grisette', 'terracotta_hedgehog', 'the_blusher', 'the_deceiver', 'the_goblet', 'the_miller', 'the_prince', 'the_sickener', 'thimble_morel', 'tripe_fungus', 'trooping_funnel', 'truffles', 'tuberous_polypore', 'turkey_tail', 'velvet_shank', 'vermillion_waxcap', 'warted_amanita', 'weeping_widow', 'white_dapperling', 'white_domecap', 'white_false_death_cap', 'white_fibrecap', 'white_saddle', 'winter_chanterelle', 'woodland_inkcap', 'wood_blewit', 'wood_mushroom', 'woolly_milkcap', 'wrinkled_peach', 'yellow_false_truffle', 'yellow_foot_waxcap', 'yellow_stagshorn', 'yellow_stainer', 'yellow_swamp_brittlegill']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"data\"\n",
    "\n",
    "# List all the folder names in the given path\n",
    "original_folder_names = [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "print(original_folder_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of img tags:  49\n",
      "Folder ./mushroom_crawling/almond_mushroom/ created!\n",
      "number of img tags:  50\n",
      "Folder ./mushroom_crawling/amanita_gemmata/ created!\n",
      "number of img tags:  49\n",
      "Folder ./mushroom_crawling/amethyst_chanterelle/ created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for keyword in original_folder_names[:3]:\n",
    "\n",
    "    lets_crawling(keyword,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder path_to_the_folder/new_folder_name created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
