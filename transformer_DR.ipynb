{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywDWUWf0t0Hf",
        "outputId": "0558c3ba-b8a9-4634-f076-a9eeb50d5fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.8-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.18.0 safetensors-0.4.0 timm-0.9.8\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm.notebook import tqdm\n",
        "import timm\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import math\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "# Set the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iZayMFyuDAK",
        "outputId": "ba161ba7-dcdb-4203-fa90-ec103e9618e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process the data\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # Geometric transformations\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Flip the image with probability=0.5\n",
        "    transforms.RandomVerticalFlip(p=0.5),  # Flip image vertically with probability=0.5\n",
        "    transforms.RandomRotation(30),  # Rotate the image up to 30 degrees\n",
        "    transforms.RandomRotation(60),  # Rotate the image up to 90 degrees\n",
        "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),  # Crop and resize\n",
        "\n",
        "    # Color transformations\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "\n",
        "    # Affine transformations\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "\n",
        "    # Convert to tensor\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Cutout augmentation\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
        "\n",
        "    # Normalize (Note: These values are standard for ImageNet. Adjust if using a different dataset)\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # This may be optional if your images are already this size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization for pretrained models on ImageNet\n",
        "])"
      ],
      "metadata": {
        "id": "UCuwDGh7uKY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed_value)  # Set the seed for torch\n",
        "    torch.cuda.manual_seed(seed_value)  # If you're using GPU\n",
        "    torch.cuda.manual_seed_all(seed_value)  # If using multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed_value)"
      ],
      "metadata": {
        "id": "HTkwf-z_JXjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all the data\n",
        "\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_root = '/content/drive/MyDrive/mushroom_data_new'\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# 1. Load all data using ImageFolder\n",
        "full_dataset = ImageFolder(data_root, transform=train_transform)\n",
        "\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)  # 70% for training\n",
        "val_size = int(0.2 * total_size)   # 20% for validation\n",
        "test_size = total_size - train_size - val_size  # 10% for testing\n",
        "\n",
        "# 3. Use random_split to split the datasets\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Optional: You might want to apply different transformations to validation and test sets (e.g., no augmentations).\n",
        "# To do this, create a function that modifies the transformations for a given subset of the dataset:\n",
        "def set_transform(dataset_subset, transform):\n",
        "    dataset_subset.dataset.transform = transform\n",
        "    return dataset_subset\n",
        "\n",
        "val_dataset = set_transform(val_dataset, val_transform)  # If you have a separate val_transform without augmentations\n",
        "test_dataset = set_transform(test_dataset, val_transform)  # Use the same as validation for simplicity\n",
        "\n",
        "# 4. Create DataLoaders for each set\n",
        "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "testloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "BybRwUOLuIUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "\n",
        "# 1. Encoder\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, nhead, feedforward_dim):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.multihead_attention = nn.MultiheadAttention(d_model, nhead)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, feedforward_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(feedforward_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.multihead_attention(x, x, x)\n",
        "        x = self.layer_norm1(x + attn_out)\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.layer_norm2(x + ff_out)\n",
        "        return x\n",
        "\n",
        "class ImagePatches(nn.Module):\n",
        "    def __init__(self, patch_size):\n",
        "        super(ImagePatches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Split images into patches\n",
        "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        patches = patches.permute(0, 2, 3, 1, 4, 5)\n",
        "\n",
        "        # Flatten patches\n",
        "        patches = patches.reshape(patches.size(0), patches.size(1) * patches.size(2), -1)\n",
        "        return patches\n",
        "\n",
        "# Load the pretrained ViT model\n",
        "vit_model = timm.create_model(\"vit_large_patch16_224\")\n",
        "num_classes = vit_model.head.in_features\n",
        "vit_model.head = torch.nn.Linear(in_features=vit_model.head.in_features, out_features=num_classes)\n",
        "\n",
        "patch_size = 16\n",
        "image_patches = ImagePatches(patch_size)\n",
        "encoder = EncoderBlock(d_model=patch_size*patch_size*3, nhead=8, feedforward_dim=2048)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vit_model.parameters(), lr=0.001)\n",
        "\n",
        "encoder = EncoderBlock(d_model=768, nhead=8, feedforward_dim=2048)  # Change dims if needed\n",
        "num_epochs = 1\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "vit_model = vit_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vit_model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        patches = image_patches(images)  # Convert images into flattened patches\n",
        "        encoded_patches = encoder(patches)\n",
        "\n",
        "        # Reshape encoded patches if needed to be compatible with ViT model\n",
        "        encoded_patches = encoded_patches.reshape(encoded_patches.size(0), 3, 224, 224)\n",
        "\n",
        "        outputs = vit_model(encoded_patches)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "    # Compute average loss and accuracy for the epoch\n",
        "    epoch_loss = total_loss / len(trainloader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2N57k5VSHJqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83S9IN1VHluQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}